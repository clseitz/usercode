%\documentclass[12pt]{article}
\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\renewcommand{\labelitemi}{$-$}
\renewcommand{\labelitemii}{$-$}
\title{ARC Comments v1}
\usepackage{color}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{xcolor}
\author{EXO-12-049}
\date{\today}



\begin{document}
\maketitle
\section{GENERAL STATEMENT}
We believe that the analysis is in a very mature state and there are no major issues. In this first set of comments we mostly address physics questions, while more detailed textual comments will be made in the second round.

The comments below are based on V4 of the PAS and V7 of the AN note.

\section{TYPE B (Physics) COMMENTS}
\begin{itemize}

\item[B1.]  We could not find any jet control plots in the AN note to show the sanity of the data sample. Please add the following distributions (data \& MC) for each of the 6 jet separately: pt, eta, phi, charged hadron/neutral hadron/photon/electron/muon energy fractions.

\textcolor{ForestGreen}{We have added kinematic plots of the six leading jets,
as well as control plots for the jet ID variables to the AN (see Figures 2 and 3).}\\

\item[B2.] Demonstrate that the HLT\_QuadJet50 trigger is suitable (unbiased) for the measurement of the signal trigger. This could be done with data, or otherwise a QCD MC study with simulated trigger would suffice.

\textcolor{ForestGreen}{We have cross-checked the result with the 19.2/fb of the JetHT data set using an "or" of two alternative triggers (Mu40 and HT200)
as the base selection and find overall good agreement.  You can see the tripper map below in Figure~\ref{fig:map}.}\\

\begin{figure}[ht]
 \begin{center}
 \includegraphics[width=0.40\textwidth]{BMu40HT200_SQuad60_Di20_2012BCD_2D-1.pdf}
         \caption{Trigger map using an or of Mu40 and HT200 from the JetHT dataset to measure the trigger efficiency for QuadJet60\_Dijet20.}
   \label{fig:map}
 \end{center}
\end{figure}


\item[B3.] Optimization of cuts
\begin{itemize}

\item The delta cut optimization is done so that the triplet-mass turn-on is pushed as low as possible. Why is this so critical, given that the masses up to 460 GeV have already been excluded? On the other hand, the 6th-jet pt optimization targets the high masses. Isn't this contradictory?

\textcolor{ForestGreen}{For the b-tagged analysis, we would like to access the kinematic region of the top quark to help validate the analysis technique, so
the low Mjjj turn-on point is needed. The sixth-jet-pT optimization does not necessarily target high masses. We just find that, for high masses, a higher pT cut improves the signal significance; whereas, for low-mass gluinos, the lower pT cut is more advantageous. We studied how the diagonal cut might be able to
enhance signal significance but found there was no optimal diagonal cut that maximized signal significance,
so we selected the diagonal cut based on the lowest possible mass range.}\\

\item It is not very clear to us what exactly are the Nsig and Nbkg in the figure of merit used for the various optimizations. Is the Nsig the Gaussian component only, or the total (Gaussian + combinatorics) of the signal in the 2-sigma window? Similarly, is Nbkg the QCD-only contribution, or the sum of QCD and combinatorics from the signal?

\textcolor{ForestGreen}{In general, changing the $p_{T}$ cut will have an effect on QCD/Data background as well as wrongly combined signal triplets, mostly
by changing the peak position of the Mjjj distribution. Also, applying b tagging will affect the composition of the data distribution (e.g., increasing the amount of ttbar), and, therefore, again the shape of the background distribution changes. For the optimization, however, we start fitting above the ttbar peak. For the jet-$p_{T}$ and b-tagging selection criteria,  we use as Nsig the number of triplets only in the Gaussian peak and as Nbkg the number of triplets in the same region of the fit to the data. Tests with QCD MC as background yield the same optimization, as shown in the AN. 
For the event shape variables, we find that a cut does not significantly change either the QCD/data or signal combinatorial distribution and, therefore, decided to use as the figure of merit Nsig being the number of expected events passing a certain sphericity cut and Nbkg being the number of events in the data passing that same cut.}\\



\item The various optimisations are performed sequentially. Is there an argument which shows that this is reasonable? (In finding the maximum of an n-dimensional function, doing n separate 1-D maximisations will not find the global maximum if the variables are correlated). Also is S/sqrt(S+B) the best approximation to optimise for the signal significance?

\textcolor{ForestGreen}{We tried the $p_{T}$-cut optimization for the different b-tagging categories and found them to be independent of each other. Figure~\ref{fig:optimization} below shows the optimization at an early stage with only 2.4/fb of data and a subset of the masses and $p_T$ cuts for different b-tagging requirements from left to right: $\geq$ 0 b, $\geq$ 1 b, and $\geq$ 2 b tags in the event. The trends are very similar as a function of mass for the different categories.}

\begin{figure}[ht]
 \begin{center}
 \hfill
 \includegraphics[width=0.30\textwidth]{kinematicopt_ge0b.pdf}
 \includegraphics[width=0.30\textwidth]{kinematicopt_ge1b.pdf}
  \includegraphics[width=0.30\textwidth]{kinematicopt_ge2b.pdf}
         \caption{
         Kinematic optimization for different b-tagging categories  $\geq$ 0 b, $\geq$ 1 b and $\geq$ 2 b tags on a small set of the data. The trend that higher masses favor a higher 6th-jet-$p_T$ cut is clearly visible in all categories.}
   \label{fig:optimization}
 \end{center}
\end{figure}

\end{itemize}

\item[B4.] We find the definition of acceptance as highly misleading, since it is the ratio of two different quantities (number of triplets over number of events). In principle, the expected number of signal triplets in the Gaussian peak would be expressed as NTgauss = (xsection * L * trig.eff * jet-id eff.) x f1 x f2 x f3, where f1 is the fraction of independent events that survive the selection and the delta cut, f2 is the average number of triplets contributed per selected event, and f3 is the fraction of the gaussian component over the total surviving triplets. It turns out that what you call "acceptance" is the product f1 x f2 x f3 (so it is mathematically correct), but it has no real physical meaning, unlike the fractions f1,f2,f3. In the approach where you only use the gaussian core, the "acceptance" is much smaller than unity (because f3 is small), and no
concerns are raised. However, when you use the full signal shape, you are left with f1 x f2 which, rightfully so, can be larger than 1 (it is the transition factor from events to triplets: if you applied no cuts, it would be exactly 20). Therefore, we find the discussion in appendix C about the "artificially raised acceptance" or "correlated triplets" as rather irrelevant: the search can be performed equally well with the full signal shape.
\begin{itemize}
\item we strongly encourage you to use a different word for what you call "acceptance" -- provide the plots of f1, f2, f3 above, as a function of the resonance mass -- how would a higher cut on delta affect f3, and the total f1 x f2 x f3 for higher signal masses? Wouldn't it reduce the combinatorics background, in favor of the gaussian peak?

\textcolor{ForestGreen}{The requested plots have been added to the AN. The "Acceptance" section has been rephrased to include your suggested definition. For higher masses, a change in diagonal cut has very little effect on f3 (the ratio of Gaussian triplets over all triplets), as can be seen in Figure~\ref{fig:diffdelta} where three different $\Delta$ cuts are chosen and f3 is shown.}

\begin{figure}[ht]
 \begin{center}
 \includegraphics[width=0.40\textwidth]{RPV112_f3_for_different_diagcuts.pdf}
         \caption{f3 is shown for different $\Delta$ cuts as a function of gluino mass. Changing $\Delta$ only has a very small effect for higher masses where the method becomes less successful in picking out a resonance peak.}
   \label{fig:diffdelta}
 \end{center}
\end{figure}

\end{itemize}

\item[B5.] From the AN note and the PAS it is not quite clear how you actually perform the fits
and the search in the light and heavy flavor cases. Please confirm the following: 
\begin{itemize}
\item[a.] for the light-flavor and heavy flavor-high mass gluino search you use a parametrized PDF (eq. 2 in AN note) for the background hypothesis, which you pass to the limit code ("combine" tool of Higgs PAG) and it takes automatically into account the fit-parameters' uncertainties and their correlations. In this case, no other background systematics is taken into account. For the signal hypothesis you use a gaussian PDF, with 2 parameters (mean \& sigma), while the signal strength is the quantity on which the limit is set. In this setup the signal combinatorial background is absorbed by the background model.

\textcolor{ForestGreen}{Yes, that is correct. We have added a few more sentences to the limit setting section to describe this more clearly.}\\

\item[b.] for the heavy-flavor, low-mass gluino extraction you use histogrammed pdfs. The background hypothesis is composed of the ttbar template from MC and the QCD template, which is taken from the data, using the b-tag vetoed dataset. The signal template is again a gaussian.
If our understanding is correct, please answer the questions below: 
\begin{itemize}
\item which binning do you use in the datacards for the gluino search? The constant one, or the one based on resolution? BTW: where did you get the 7\% resolution for mjjj? 

\textcolor{ForestGreen}{We use the 10 GeV bins for the limit setting. The 7\% comes from the combination of the three jets. We use this as an average as is done with the measurement of the three-jet mass cross section in SMP-12-027.}\\

\item in case b, how do you deal with the combinatorial background from the signal? Apparently it cannot be absorbed by the QCD template now (because it is taken from the data control region). 

\textcolor{ForestGreen}{The $p_T$ spectrum of low-mass gluinos is fairly similar to the one from QCD; therefore, the wrongly combined signal triplet distribution  and the QCD Mjjj distributions are very similar. We have shown 
in a closure test for ttbar that the 0-b data can absorb the wrongly combined triplets of the signal since the overall scaling is allowed to float freely. The test can be found in Figure~\ref{fig:ttbar}.}\\
\begin{figure}[ht]
 \begin{center}
 \includegraphics[width=0.75\textwidth]{ttbar_show_method.pdf}
         \caption{Closure test that QCD background could cover wrongly combined  triplets for low-mass signal in the case of measuring ttbar production.
         The r values for both cases (Gaussian model and full signal shape) agree within uncertainties.}
   \label{fig:ttbar}
 \end{center}
\end{figure}
\item produce a MC closure plot showing that the mjjj template from the b-tag vetoed QCD events agrees well with the one of the signal region.

\textcolor{ForestGreen}{We have a similar plot in the analysis note in Appendix E, Figure 47, where the triplet invariant mass distribution is shown in yellow for the inclusive case ($\geq$ 0 b jets) and overlaid in red 
for the QCD MC distribution with at least 1 b jet in the event. Below in Figure~\ref{fig:bvetoe} you can see the comparison of the Mjjj distributions for b-vetoed triplets and for $\geq$ 1 b tag in the triplet (matching the event selection for the low-mass heavy-flavor search).}\\


\begin{figure}[ht]
 \begin{center}
 \includegraphics[width=0.40\textwidth]{bvetoe_QCD.pdf}
         \caption{Comparison of the Mjjj distribution for b-vetoed triplets and with $\geq$ 1 b tag in the triplet (matching the event selection for the low-mass heavy-flavor search)}
   \label{fig:bvetoe}
 \end{center}
\end{figure}

\end{itemize}
\end{itemize}
\item[B6.] Systematic uncertainties: 
\begin{itemize}
\item have you assigned a systematic uncertainty on the signal acceptance due to PDFs? 
\textcolor{red}{Work in Progress. (Dan)}\\
\item the JES \& JER uncertainties have been assigned on the signal acceptance, but have you considered their effect on the signal peak position and gaussian sigma? The latter should enter the datacards and the workspace that contain the signal PDFs.

\textcolor{ForestGreen}{To account for the uncertainty on the JER, we assign a 10\% uncertainty on the width of the Gaussian, not just on the acceptance. After your suggestion, we did evaluate the effect of JES on the Gaussian mean and find it to be around 1\% for all masses. This uncertainty
is now included in the datacards for the limit setting.}\\

\end{itemize}

\item[B7.] Signal extraction with toys -- in the AN note, Appendix C4, you describe the signal injection \& fit procedure. You take as background the P4 fit of the data, and for the extraction you use again the P4. By construction then, if things are done properly, this procedure yields the correct signal strength. Have you tried to generate the background with a different model than P4, and fit with P4? Perhaps you could use the QCD background shape directly from the MC, and normalized to the data, as the generating template.

\textcolor{ForestGreen}{You are correct that the background is determined from the P4 model and then refit with the P4. However, it is important to note that  we inject the full shape signal on top of this background, which gives a contribution to almost every single bin, altering the complete shape.  Then 
we fit with P4$+$Gauss or P4$+$full shape, and the P4 parameters are allowed to float freely without including any previous knowledge of the background parameters. In the AN in section C4, we fit the same toy histogram with a P4 plus the two different signal shapes and find that the parameters adjust accordingly and give very similar results for the signal strength.}\\

\item[B8.] Do you understand why the full signal shape analysis yields better limits?

\textcolor{ForestGreen}{The full-shape analysis seems to include some additional
information. Also, the method of constructing the expected number of triplets with the full-shape technique creates a higher expected signal for each mass point.}\\

\end{itemize}
\section{PAS SPECIFIC COMMENTS}
\begin{itemize}
\item Abstract line 4: Presumably the different models result in different acceptances, even for a given resonance mass. Does this still make it 'model independent'?

\textcolor{ForestGreen}{Our search is sensitive to any significant bump on our
smooth background. Our limits are model-dependent, but the fact that the data
matches the background estimate shows that there is no evidence of new physics
of any type.}\\

\item line 9: different technique $==>$ better to say a little bit clearly.

\textcolor{ForestGreen}{Text clarified.}\\

\item line 42: Should it be 100\%/sqrt(E), rather than 100\% * sqrt(E) ?

\textcolor{ForestGreen}{Fixed.}\\

\item Fig 1: a) Why is the red line shown at 100 GeV, when the value used is 110? b)"flavor = 112" (113\_223 in Fig.2 also) $==>$ we couldn't find an explanation of these legend clearly in the text

\textcolor{ForestGreen}{We have remade this plot and fixed the position of the diagonal cut.}\\

\item line 73: Two different scenarios... $==>$ Any particular reasons to choose these two scenarios?,

\textcolor{ForestGreen}{The text has been clarified to indicate that the
light-flavor search provides comparison with previous similar searches,
while the heavy-flavor search extends these searches for the first time to
include b jets.}\\


\item line 75: ... one b jet and two light-flavor jets $==>$ How about two b jets plus one light jet, for example.

\textcolor{ForestGreen}{The RPV coupling is u\_i d\_j d\_k  (in our case 112 or 113\_223) where i,j,k and
generation indices, and j and k are antisymmetric.
For decay of an SU(3)\_C adjoint fermion, the antisymmetry is a required by gauge invariance.
This means that there can be only at most one b-quark (direct) in the decay.}\\

\item line 76-77: As far as we understand all high mass samples are produced privately. So, it would be nice to compare your sample with a few centrally produced sample for sanity check.

\textcolor{ForestGreen}{This check has been added to the analysis note, and we find good agreement between 
centrally and private produced samples.}

\item line 78: ... the natural width ... of the detector $==>$ Is this always true? no resonance mass dependence?

\textcolor{ForestGreen}{The signal MC samples were generated with negligible
resonance widths. The reconstructed signal width is due purely to 
detector resolution.}\\


\item line 115: Maybe change 'signal sensitivity' to 'signal efficiency' (reducing the background would result in increased signal sensitivity)

\textcolor{ForestGreen}{Fixed.}\\

\item Tables in Fig 2 and Fig 3: Does the normalisation really help? It fails to show that no 6th jet cut gives any chance of seeing a signal at high gluino masses. Make the text above the tables in the two diagrams compatible.

\textcolor{ForestGreen}{The normalization helps to
show the trend in signal significance over the range of masses. If the
unnormalized significance was shown, the trend would be less clear to the reader. The table headers have been fixed.}\\

\item Paragraph right after the Fig.2 caption: Its medium operating point... $==>$ Better to have more info about medium operating point.

\textcolor{ForestGreen}{Additional detail added to text.}\\

\item Eqn 2: We assume that alpha and beta are the components wrt two axes; and that the choice of axes is irrelevant for the extraction of the eigenvalues. Is this correct?

\textcolor{ForestGreen}{Yes, you are correct.}\\

\item Eqn 3: Did you try other parameter functions?

\textcolor{ForestGreen}{The previous 2011 analysis, EXO-11-060, studied two other
fit functions and found that they gave very similar results.}\\

\item Fig 4 and 5: a) dN/dMjjj has units b) It is better to give chi$^2$ and ndf, rather than just chi$^2$\/ndf c) Is chi$^2$ calculated using the likelihood parameters, and is it calculated from the likelihood by Baker and Cousins, or in a conventional chi$^2$ way? The latter would have problems with the bins with very few events. d) In fig 4, does something funny happen at 1000 GeV (e.g. change in event selection), or is the jump a statistical fluctuation?

\textcolor{ForestGreen}{The fits are performed on 10-GeV-bin-width distributions and then overlaid on the resolution-based
binned histograms. The chi$^2$ is also determined from the original fit with the classical implementation in Root. The numbers shown
give an estimate that the fits are giving reasonable results. All triplets in this plot have the same event and triplet selection applied, so there is no change in selection around 1 TeV. The small shift at 1 TeV appears to be a statistical fluctuation.}\\

\item line 128: ... contribute minimally $==>$ Where can we find a supporting sentence(s) for this argument in the text?

\textcolor{ForestGreen}{This has been rephrased. We find that the shape of wrong combinations is very similar
to the one from QCD. With a toy study, we have shown that the background P4 from the data would cover both QCD background
as well as any possible wrongly combined signal triplets.}\\

\item line 132: the underlying kinematic distributions are .. $==>$ What kinds of distributions did you look at? Triplet invariant mass distribution and what else?

\textcolor{ForestGreen}{This confusing sentence has been deleted. It was a left-over from a previous iteration of the PAS.}\\

\item line 135: When b tag ... $==>$ How did you control (or treat) the ISR jet from ttbar or QCD multijet production?

\textcolor{ForestGreen}{If an ISR jet is among the six highest-$p_T$ jets in the event, then it is still treated like every other jet and included
in the triplets. We take into account uncertainties on ISR/FSR for ttbar by using dedicated samples that model this systematic.}\\


\item line 143: What are we supposed to deduce from the value of r? That tt is suppressed? That tt is OK if systematics are included?........

\textcolor{ForestGreen}{The text has been clarified to say that the determination of r acts as a validation of the analysis technique.}\\

\item Fig 6, right plot: It would be more useful to plot the offset versus mass.

\textcolor{ForestGreen}{We have removed the plot since it goes down to an
inappropriate level of detail for the PAS without providing relevant
information for the reader. The plots of acceptance and width
as functions of mass show the signal model is well-described.}\\

\item Fig.7: It would be nice to see the final limit with full shape including signal systematics.

\textcolor{ForestGreen}{We consider the full-shape analysis as a cross check and not a final
result and thus would prefer not to include it in the PAS.}\\

\item lines 177-8 and 190-1: Is that a significant difference in the wording for the ranges quoted for gluinos decaying to light and heavy quarks? The former are excluded BELOW 770, while the latter are excluded BETWEEN 200 and 860 GeV.

\textcolor{ForestGreen}{We have updated the limit plots to now include larger errors on the background parameter uncertainties (as requested by the conveners during pre-approval) and also include the uncertainty on the mean position of the Gaussian due to JES. Additionally,
we added more mass points where the fit is performed. Following the SUSY groups recommendation, we added $\pm$ 1 $\sigma$ error bands on the theory cross section, and we now quote the more conservative limit. The previous light-flavor analysis excluded gluinos below 460 GeV, and our
results extend that exclusion up to 650 GeV. But there are no previous heavy-flavor results,
and our analysis is not sensitive below 200 GeV, so that is why we specify a mass range for
the heavy-flavor results. The text has been rephrased to clarify this point.}\\

\end{itemize}
\section{TYPE A (textual) COMMENTS on the PAS}
\begin{itemize}
\item ABSTRACT, line 6:	Delete one of the two 'includes'.

\textcolor{ForestGreen}{Fixed.}\\

\item line 9: It is not obvious that 'respectively' applies to the Tevatron and LHC.

\textcolor{ForestGreen}{Text clarified.}\\

 \item Fig 2 caption, bottom line: Change 'bottom row' to 'table'.

\textcolor{ForestGreen}{Fixed.}\\


\item Fig2 caption, line 3: '.... from selectiNG events....' 

\textcolor{ForestGreen}{Fixed.}\\

\item line 142: '... can be extractED relative....' 

\textcolor{ForestGreen}{Fixed.}\\

\item Eqn 3: Put this higher in the text (e.g. line 134) 

\textcolor{ForestGreen}{Fixed.}\\

\item line 164: 'sensitive TO uncertainties Affecting..... events. thesE' 

\textcolor{ForestGreen}{Fixed.}\\

\item Ref [24]: Why are 'MCFM' and 'LHC' in curly brackets?

\textcolor{ForestGreen}{This error has been fixed.}\\

\end{itemize}
\section{AN NOTE SPECIFIC COMMENTS}
\begin{itemize}
\item Table 2: Too many sig figs for the cross-section

\textcolor{ForestGreen}{Fixed.}\\

\item line 113: we still don't see why the mass of background triplets must scale with p\_T. The PAS actually uses what we regard as better wording for this.

\textcolor{ForestGreen}{Wording changed to follow the version in the PAS.}\\

\item line 119: ``The horizontal band ..." As has always been the case for this analysis, it is essentially impossible to see the horizontal band since it is overwhelmed by the distribution of background points. It would be good to somehow make the band more visible, e.g., by using a different color for points that represent correctly paired triplets. Same thing in Fig.1 of the PAS.

\textcolor{ForestGreen}{A new plot showing this more clearly has been added to the PAS.}\\

\item Eqn 2: Is this the Landau distribution, referred to later (e.g. Fig 7 caption)?

\textcolor{ForestGreen}{Eqn. 2 shows the four-parameter fit function (also refereed to as P4) that is used for the background estimate. The caption of Fig. 7 had a mistake: It should have read "P4" instead of "Landau". The Landau function itself is only used to estimate the peak position of Mjjj in the data but not to actually model the background itself.}\\

\item Fig.1 caption: ``The left graph" should be ``The right graph"

\textcolor{ForestGreen}{Fixed.}\\

\item Figs.3, 4: Are these events signal events only (with the wrong-combination background) or do they include QCD background as well. It would be good to make this clear in the figure captions, similar to what is done in Fig.6. Same thing in Fig.1 of the PAS.

\textcolor{ForestGreen}{These plots show signal events only. The captions have been clarified.}\\

\item Fig 7, right plot: Is this fit for background and just Gaussian signal (or signal = Gauss plus combinatorics)?

\textcolor{ForestGreen}{The signal shape is only the Gaussian signal peak, while the background
fit comes only from data. The caption has been rephrased for clarity.}\\


\item Fig 8: The significances are quoted to too many sig figs. (e.g. 282.356) In caption, line 3: '....The bottom PAIR OF TABLES represent.....

\textcolor{ForestGreen}{The caption has been fixed. Because of the large range of values from 0.093 to over 200, it was necessary to include
more digits. The table shown is a Root plot, and only one option can be set to specify the number of digits shown with the "text" drawing option.}\\

\item line 175 ``...signal Gaussian and background are combined..." Does ``background" mean the QCD background? What about the background from the signal-event wrong combinations? Same thing in Fig.12. Shouldn't the signal-event wrong combinations be included?

\textcolor{ForestGreen}{For all optimization purposes, "background" refers to the estimate obtained from a fit directly to the data using the P4 function, which assumes a smoothly falling distribution. The optimization mainly depends
on how the cuts change the shape and distribution of QCD background with respect
to the fixed position of the gluino resonance mass. As discussed in previous
answers, the wrongly combined signal triplet distribution is very similar to 
that of the QCD background.}\\

\item line 196: Here it refers to QCD MC giving the same optimisation. What was used before?

\textcolor{ForestGreen}{The data itself is used for the optimization studies; however, we have also tested QCD Monte Carlo, which gives the same answers.}\\

\item Section 6.6: could you explain the Madgraph-Pythia matching issue in one sentence (also in the PAS).

\textcolor{ForestGreen}{An explanation has been added to the AN and PAS.}\\

\item line 417: Are the correlations between the uncertainties in the fit parameters also used?

\textcolor{ForestGreen}{We are not explicitly taking the correlations on the uncertainties into account. These correlations were studied in the 2011 analysis, and the effect on the limits
was very small.}\\

\item line 544: '.... Section 4.2 and 4.2....' 
line 568: '.... the the .......' 
line 569: Change first '(right)' to '(left)' 
line 590: '.....The MIDDLE row of...'

\textcolor{ForestGreen}{All fixed.}\\

\end{itemize}
\end{document}
